{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# extract the json from inside the HTML files\n",
    "def extract_json():\n",
    "    path = \"./data/detail\"\n",
    "    count = 0\n",
    "    details_dict = {}\n",
    "\n",
    "    # https://stackoverflow.com/questions/22394235/invalid-control-character-with-python-json-loads\n",
    "\n",
    "    for name in os.listdir(path):\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Progress: {count}\")\n",
    "        with open(os.path.join(path, name), 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f)\n",
    "            data = json.loads(soup.find(\"script\", type=\"application/ld+json\").contents[0], strict=False)\n",
    "            details_dict[name.split(\".\")[0]] = data\n",
    "\n",
    "    return details_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "4289"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_dict_path = \"./data/details_dict.pkl\"\n",
    "\n",
    "if os.path.isfile(details_dict_path):\n",
    "    with open(details_dict_path, \"rb\") as f:\n",
    "        details_dict = pickle.load(f)\n",
    "else:\n",
    "    details_dict = extract_json()\n",
    "    with open(\"./data/details_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(details_dict, f)\n",
    "\n",
    "len(details_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "10833"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celebrities = set()\n",
    "\n",
    "for data in details_dict.values():\n",
    "    for d in data[\"director\"][0:1]:\n",
    "        celebrities.add(d[\"url\"])\n",
    "    for a in data[\"author\"][0:2]:\n",
    "        celebrities.add(a[\"url\"])\n",
    "    # only pick the lead actors otherwise too many to scrape\n",
    "    for a in data[\"actor\"][0:4]:\n",
    "        celebrities.add(a[\"url\"])\n",
    "\n",
    "len(celebrities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张艺谋\n",
    "\"/celebrity/1054398/\" in celebrities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jay Chou\n",
    "\"/celebrity/1048000/\" in celebrities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 章子怡\n",
    "\"/celebrity/1041014/\" in celebrities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 10 thousand... this will take some time\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from common import get_headers, headers, proxies\n",
    "import random\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# cf. 00c\n",
    "\n",
    "def get_next_proxy():\n",
    "    try:\n",
    "        return f\"http://{next(get_next_proxy.proxy_it)}\"\n",
    "    except StopIteration:\n",
    "        get_next_proxy.proxy_it = iter(proxies)\n",
    "        print(\"Proxies list exhausted, reverting to None\")\n",
    "        return None\n",
    "\n",
    "get_next_proxy.proxy_it = iter(proxies)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong response code: 500\n",
      "Something went wrong, pausing scrape for a minute\n"
     ]
    }
   ],
   "source": [
    "proxy = None\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "\n",
    "async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=60)) as session:\n",
    "    for c in celebrities:\n",
    "        i = c.split('/')[2]\n",
    "        name = f\"./data/celeb/{i}.html\"\n",
    "        count += 1\n",
    "        if os.path.isfile(name):\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                async with session.get(\"https://movie.douban.com\" + c, proxy=proxy) as resp:\n",
    "                    if resp.status == 200:\n",
    "                        result = await resp.text()\n",
    "                        if not result.startswith(\"<script>\"):\n",
    "                            with open(name, 'w', encoding=\"utf-8\") as f:\n",
    "                                f.write(result)\n",
    "                            break\n",
    "                        print(\"Response starts with script tag\")\n",
    "                    else:\n",
    "                        print(f\"Wrong response code: {resp.status}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e}\")\n",
    "            # proxy = get_next_proxy()\n",
    "            # print(f\"Something went wrong, switching proxy to {proxy}\")\n",
    "            print(\"Something went wrong, pausing scrape for a minute\")\n",
    "            time.sleep(60)\n",
    "\n",
    "        print(f\"Scraped {i}, speed {time.time() - start_time}, {len(celebrities) - count} to go\")\n",
    "        start_time = time.time()\n",
    "        time.sleep(40 * random.random() + 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "10831"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of scraped files is correct\n",
    "_, _, files = next(os.walk(\"./data/celeb\"))\n",
    "len(files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parallel Approach\n",
    "Below I tried using a parallel approach that, instead of rotating proxies, runs an array of concurrent jobs that spams all the proxies at once, removing jobs from a \"queue\" (in fact a set) as necessary. Unfortunately many proxies obtained from the internet were non-functional, otherwise this approach would be significantly faster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove from set those already scraped\n",
    "already_scraped = []\n",
    "\n",
    "for c in celebrities:\n",
    "    i = c.split('/')[2]\n",
    "    name = f\"./data/celeb/{i}.html\"\n",
    "    if os.path.isfile(name):\n",
    "        already_scraped.append(c)\n",
    "\n",
    "for c in already_scraped:\n",
    "    celebrities.remove(c)\n",
    "\n",
    "len(celebrities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'/celebrity/1314395/', '/celebrity/1316349/'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these two lead to http 500 errors\n",
    "celebrities\n",
    "# for these two we can use the mobile website version i.e. https://m.douban.com/movie/celebrity/1316349/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_next_celeb():\n",
    "    celeb_it = iter(celebrities)\n",
    "    return next(celeb_it)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "async def proxy_get(proxy, session):\n",
    "    h = get_headers()\n",
    "    while len(celebrities) > 0:\n",
    "        celeb = get_next_celeb()\n",
    "        i = celeb.split('/')[2]\n",
    "        name = f\"./data/celeb/{i}.html\"\n",
    "        try:\n",
    "            async with session.get(\"https://movie.douban.com\" + celeb, headers=h, proxy=f\"http://{proxy}\") as resp:\n",
    "                if resp.status == 200:\n",
    "                    result = await resp.text()\n",
    "                    if not result.startswith(\"<script>\"):\n",
    "                        with open(name, 'w', encoding=\"utf-8\") as f:\n",
    "                            f.write(result)\n",
    "                        celebrities.remove(celeb)\n",
    "                        print(f\"Scraped {i}, proxy {proxy}, {len(celebrities)} left\")\n",
    "                        continue\n",
    "                print(f\"Error: {resp.status}, proxy: {proxy}\")\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"Something went wrong, pausing scrape for {proxy}\")\n",
    "        # cannot be time.sleep here because that's blocking\n",
    "        await asyncio.sleep(300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong, pausing scrape for 05.189.229.42:1081\n",
      "Something went wrong, pausing scrape for 80.48.119.28:8080\n",
      "Something went wrong, pausing scrape for 74.208.205.5:80\n",
      "Something went wrong, pausing scrape for 169.57.1.85:8123\n",
      "Something went wrong, pausing scrape for 50.87.181.51:80\n",
      "Something went wrong, pausing scrape for 157.100.26.69:80\n",
      "Something went wrong, pausing scrape for 51.79.50.31:9300\n",
      "Something went wrong, pausing scrape for 67.212.186.100:80\n",
      "Something went wrong, pausing scrape for 80.179.140.189:80\n",
      "Something went wrong, pausing scrape for 216.137.184.253:80\n",
      "Something went wrong, pausing scrape for 151.106.17.124:1080\n",
      "Something went wrong, pausing scrape for 151.106.17.125:1080\n",
      "Something went wrong, pausing scrape for 151.106.17.126:1080\n",
      "Something went wrong, pausing scrape for 196.1.95.117:80\n",
      "Something went wrong, pausing scrape for 119.13.124.196:59394\n",
      "Something went wrong, pausing scrape for 151.106.17.123:1080\n",
      "Something went wrong, pausing scrape for 187.217.54.84:80\n",
      "Something went wrong, pausing scrape for 222.92.207.98:40080\n",
      "Error: 403, proxy: 167.235.63.238:3128\n",
      "Something went wrong, pausing scrape for 167.235.63.238:3128\n",
      "Something went wrong, pausing scrape for 151.106.17.122:1080\n",
      "Something went wrong, pausing scrape for 158.69.71.245:9300\n",
      "Something went wrong, pausing scrape for 41.223.136.162:80\n",
      "Something went wrong, pausing scrape for 27.184.51.17:8089\n",
      "Something went wrong, pausing scrape for 151.106.18.126:1080\n",
      "Error: 403, proxy: 124.226.194.135:808\n",
      "Something went wrong, pausing scrape for 124.226.194.135:808\n",
      "Error: 500, proxy: 68.183.185.62:80\n",
      "Something went wrong, pausing scrape for 68.183.185.62:80\n",
      "Something went wrong, pausing scrape for 67.212.186.99:80\n",
      "Something went wrong, pausing scrape for 43.255.113.232:8084\n",
      "Error: 403, proxy: 66.94.97.238:443\n",
      "Something went wrong, pausing scrape for 66.94.97.238:443\n",
      "Something went wrong, pausing scrape for 133.242.146.103:8000\n",
      "Something went wrong, pausing scrape for 138.201.108.13:3389\n",
      "Error: 500, proxy: 128.199.155.145:3128\n",
      "Something went wrong, pausing scrape for 128.199.155.145:3128\n",
      "Something went wrong, pausing scrape for 67.212.186.102:80\n",
      "Error: 403, proxy: 194.233.69.126:443\n",
      "Something went wrong, pausing scrape for 194.233.69.126:443\n",
      "Something went wrong, pausing scrape for 34.145.226.144:8080\n",
      "Something went wrong, pausing scrape for 51.38.191.151:80\n",
      "Something went wrong, pausing scrape for 67.63.33.7:80\n",
      "Something went wrong, pausing scrape for 151.181.90.10:80\n",
      "Something went wrong, pausing scrape for 151.181.91.10:80\n",
      "Something went wrong, pausing scrape for 222.214.191.24:9999\n",
      "Something went wrong, pausing scrape for 188.75.64.38:3128\n",
      "Something went wrong, pausing scrape for 38.127.173.47:29602\n",
      "Something went wrong, pausing scrape for 145.239.169.41:1080\n",
      "Something went wrong, pausing scrape for 87.103.175.250:9812\n",
      "Something went wrong, pausing scrape for 142.93.223.219:8080\n",
      "Something went wrong, pausing scrape for 129.213.183.152:80\n",
      "Something went wrong, pausing scrape for 51.91.62.219:80\n",
      "Something went wrong, pausing scrape for 145.239.169.44:1080\n",
      "Something went wrong, pausing scrape for 144.91.123.26:80\n",
      "Something went wrong, pausing scrape for 103.105.212.106:53281\n",
      "Something went wrong, pausing scrape for 118.67.151.29:80\n",
      "Something went wrong, pausing scrape for 85.70.210.30:80\n",
      "Something went wrong, pausing scrape for 118.31.1.154:80\n",
      "Something went wrong, pausing scrape for 74.205.128.200:80\n",
      "Something went wrong, pausing scrape for 146.59.83.187:80\n",
      "Something went wrong, pausing scrape for 200.105.215.18:33630\n",
      "Something went wrong, pausing scrape for 58.212.197.222:81\n",
      "Something went wrong, pausing scrape for 51.79.50.46:5566\n",
      "Something went wrong, pausing scrape for 196.1.97.209:80\n",
      "Something went wrong, pausing scrape for 145.239.169.40:1080\n",
      "Something went wrong, pausing scrape for 46.127.117.48:3128\n",
      "Something went wrong, pausing scrape for 145.239.169.47:1080\n",
      "Something went wrong, pausing scrape for 201.217.49.2:80\n",
      "Something went wrong, pausing scrape for 3.22.79.82:5566\n",
      "Something went wrong, pausing scrape for 88.1.165.103:3128\n",
      "Something went wrong, pausing scrape for 103.8.249.97:80\n",
      "Something went wrong, pausing scrape for 185.156.98.69:30400\n",
      "Something went wrong, pausing scrape for 203.77.231.90:9812\n",
      "Something went wrong, pausing scrape for 123.63.213.137:80\n",
      "Something went wrong, pausing scrape for 95.111.243.221:5566\n",
      "Something went wrong, pausing scrape for 134.122.26.11:80\n",
      "Something went wrong, pausing scrape for 185.51.10.19:80\n",
      "Something went wrong, pausing scrape for 129.213.69.94:80\n",
      "Something went wrong, pausing scrape for 176.99.2.43:1081\n",
      "Something went wrong, pausing scrape for 200.37.140.37:10101\n",
      "Error: 500, proxy: 58.23.212.52:3129\n",
      "Something went wrong, pausing scrape for 58.23.212.52:3129\n",
      "Something went wrong, pausing scrape for 85.195.104.71:80\n",
      "Something went wrong, pausing scrape for 37.18.73.85:5566\n",
      "Something went wrong, pausing scrape for 152.44.40.139:5566\n",
      "Something went wrong, pausing scrape for 194.233.69.90:443\n",
      "Something went wrong, pausing scrape for 40.70.172.110:80\n",
      "Something went wrong, pausing scrape for 58.137.62.133:80\n",
      "Something went wrong, pausing scrape for 185.15.172.212:3128\n",
      "Something went wrong, pausing scrape for 120.26.14.114:8888\n",
      "Something went wrong, pausing scrape for 5.189.184.6:80\n",
      "Something went wrong, pausing scrape for 58.23.212.10:3129\n",
      "Something went wrong, pausing scrape for 43.204.244.135:80\n",
      "Something went wrong, pausing scrape for 139.198.157.59:7890\n",
      "Something went wrong, pausing scrape for 152.44.42.188:5566\n",
      "Something went wrong, pausing scrape for 194.233.69.38:443\n",
      "Something went wrong, pausing scrape for 158.69.53.132:5566\n",
      "Something went wrong, pausing scrape for 66.94.120.161:443\n",
      "Something went wrong, pausing scrape for 194.233.69.41:443\n",
      "Something went wrong, pausing scrape for 158.69.27.94:5566\n",
      "Something went wrong, pausing scrape for 194.233.67.98:443\n",
      "Something went wrong, pausing scrape for 51.79.50.22:9300\n",
      "Something went wrong, pausing scrape for 173.249.57.9:443\n",
      "Something went wrong, pausing scrape for 198.144.159.40:3128\n",
      "Something went wrong, pausing scrape for 103.68.62.11:80\n",
      "Something went wrong, pausing scrape for 159.65.69.186:9300\n",
      "Something went wrong, pausing scrape for 35.233.16.212:80\n",
      "Something went wrong, pausing scrape for 218.78.54.149:8901\n",
      "Something went wrong, pausing scrape for 103.37.141.69:80\n",
      "Something went wrong, pausing scrape for 103.20.204.104:80\n",
      "Something went wrong, pausing scrape for 40.117.254.183:80\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mCancelledError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\tasks.py:605\u001B[0m, in \u001B[0;36msleep\u001B[1;34m(delay, result)\u001B[0m\n\u001B[0;32m    604\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 605\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m future\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mCancelledError\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mCancelledError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m proxies:\n\u001B[0;32m      5\u001B[0m     futures\u001B[38;5;241m.\u001B[39mappend(proxy_get(p, session))\n\u001B[1;32m----> 6\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m*\u001B[39mfutures)\n",
      "\u001B[1;31mCancelledError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# using a parallel instead of sequential approach in 00c\n",
    "async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=60)) as session:\n",
    "    futures = []\n",
    "    for p in proxies:\n",
    "        futures.append(proxy_get(p, session))\n",
    "    results = await asyncio.gather(*futures)\n",
    "    # print(f\"{sum(results)}/{len(results)} successes in pass, time taken {time.time() - start_time}, {len(celebrities)} celebrities to go\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}