{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# extract the json from inside the HTML files\n",
    "def extract_json():\n",
    "    path = \"./data/detail\"\n",
    "    count = 0\n",
    "    details_dict = {}\n",
    "\n",
    "    # https://stackoverflow.com/questions/22394235/invalid-control-character-with-python-json-loads\n",
    "\n",
    "    for name in os.listdir(path):\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Progress: {count}\")\n",
    "        with open(os.path.join(path, name), 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f)\n",
    "            data = json.loads(soup.find(\"script\", type=\"application/ld+json\").contents[0], strict=False)\n",
    "            details_dict[name.split(\".\")[0]] = data\n",
    "\n",
    "    return details_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "4289"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_dict_path = \"./data/details_dict.pkl\"\n",
    "\n",
    "if os.path.isfile(details_dict_path):\n",
    "    with open(details_dict_path, \"rb\") as f:\n",
    "        details_dict = pickle.load(f)\n",
    "else:\n",
    "    details_dict = extract_json()\n",
    "    with open(\"./data/details_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(details_dict, f)\n",
    "\n",
    "len(details_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "10833"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celebrities = set()\n",
    "\n",
    "for data in details_dict.values():\n",
    "    for d in data[\"director\"][0:1]:\n",
    "        celebrities.add(d[\"url\"])\n",
    "    for a in data[\"author\"][0:2]:\n",
    "        celebrities.add(a[\"url\"])\n",
    "    # only pick the lead actors otherwise too many to scrape\n",
    "    for a in data[\"actor\"][0:4]:\n",
    "        celebrities.add(a[\"url\"])\n",
    "\n",
    "len(celebrities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张艺谋\n",
    "\"/celebrity/1054398/\" in celebrities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jay Chou\n",
    "\"/celebrity/1048000/\" in celebrities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 章子怡\n",
    "\"/celebrity/1041014/\" in celebrities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 10 thousand... this will take some time\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from common import headers, proxies\n",
    "import random\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "8896"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove from set those already scraped\n",
    "already_scraped = []\n",
    "\n",
    "for c in celebrities:\n",
    "    i = c.split('/')[2]\n",
    "    name = f\"./data/celeb/{i}.html\"\n",
    "    if os.path.isfile(name):\n",
    "        already_scraped.append(c)\n",
    "\n",
    "for c in already_scraped:\n",
    "    celebrities.remove(c)\n",
    "\n",
    "len(celebrities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "async def get_celeb(proxy, celeb, session):\n",
    "    i = celeb.split('/')[2]\n",
    "    name = f\"./data/celeb/{i}.html\"\n",
    "    try:\n",
    "        async with session.get(\"https://movie.douban.com\" + celeb, proxy=f\"http://{proxy}\") as resp:\n",
    "            if resp.status == 200:\n",
    "                result = await resp.text()\n",
    "                if not result.startswith(\"<script>\"):\n",
    "                    with open(name, 'w', encoding=\"utf-8\") as f:\n",
    "                        f.write(result)\n",
    "                    celebrities.remove(celeb)\n",
    "                    print(f\"Scraped {i}, proxy {proxy}\")\n",
    "                    return True\n",
    "            print(f\"Error: {resp.status}, proxy: {proxy}\")\n",
    "    except:\n",
    "        pass\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 200, proxy: 175.44.46.3:8118\n",
      "Error: 200, proxy: 139.9.64.238:443\n",
      "Scraped 1023418, proxy 47.99.133.26:3128\n",
      "Error: 200, proxy: 119.122.212.20:9000\n",
      "Scraped 1340456, proxy 47.100.45.114:80\n",
      "Error: 403, proxy: 123.56.216.85:22\n",
      "Error: 403, proxy: 39.96.9.1:8080\n",
      "Scraped 1333535, proxy 113.87.81.187:8118\n",
      "Scraped 1315463, proxy 39.107.91.243:22\n",
      "Error: 403, proxy: 122.70.157.11:808\n",
      "Scraped 1318284, proxy 58.212.197.222:81\n",
      "Error: 200, proxy: 114.115.181.74:8080\n",
      "Error: 403, proxy: 221.215.252.88:9999\n",
      "Error: 403, proxy: 39.106.69.122:3128\n",
      "Scraped 1324524, proxy 58.23.212.10:3129\n",
      "6 successes in pass, time taken 60.474003076553345, 8890 movies to go\n",
      "Error: 200, proxy: 175.44.46.3:8118\n",
      "Error: 200, proxy: 139.9.64.238:443\n",
      "Scraped 1407741, proxy 116.22.31.201:8118\n",
      "Scraped 1318295, proxy 58.212.197.222:81\n",
      "Scraped 1332055, proxy 120.79.136.134:8080\n",
      "Scraped 1369033, proxy 112.232.109.1:8118\n",
      "Error: 403, proxy: 221.215.252.88:9999\n",
      "Error: 403, proxy: 39.96.9.1:8080\n",
      "Error: 403, proxy: 122.70.157.11:808\n",
      "Error: 403, proxy: 111.1.139.96:80\n",
      "Error: 200, proxy: 222.214.191.24:9999\n",
      "Scraped 1304551, proxy 139.227.201.203:8118\n",
      "Error: 403, proxy: 123.56.216.85:22\n",
      "Error: 403, proxy: 121.236.163.177:888\n",
      "Error: 200, proxy: 114.115.181.74:8080\n",
      "Error: 403, proxy: 117.147.111.210:3128\n",
      "Error: 403, proxy: 116.20.228.108:3128\n",
      "5 successes in pass, time taken 60.99707293510437, 8885 movies to go\n",
      "Error: 200, proxy: 222.214.191.24:9999\n",
      "Error: 200, proxy: 119.122.212.20:9000\n",
      "Error: 200, proxy: 175.44.46.3:8118\n",
      "Error: 403, proxy: 221.215.252.88:9999\n",
      "Error: 200, proxy: 139.9.64.238:443\n",
      "Error: 403, proxy: 116.22.31.201:8118\n",
      "Error: 403, proxy: 39.96.9.1:8080\n",
      "Error: 403, proxy: 121.196.100.243:3128\n",
      "Error: 403, proxy: 123.56.216.85:22\n",
      "Error: 403, proxy: 222.64.109.23:9000\n",
      "Error: 403, proxy: 122.70.157.11:808\n",
      "Error: 403, proxy: 111.1.139.96:80\n",
      "Error: 403, proxy: 58.23.212.10:3129\n",
      "Error: 403, proxy: 121.89.245.93:22\n",
      "Error: 200, proxy: 114.115.181.74:8080\n",
      "Error: 403, proxy: 117.147.111.210:3128\n",
      "Error: 403, proxy: 39.106.69.122:3128\n",
      "0 successes in pass, time taken 61.005455017089844, 8885 movies to go\n",
      "Error: 200, proxy: 117.80.127.28:888\n",
      "Error: 200, proxy: 139.9.64.238:443\n",
      "Error: 200, proxy: 222.214.191.24:9999\n",
      "Error: 200, proxy: 119.122.212.20:9000\n",
      "Error: 403, proxy: 39.96.9.1:8080\n",
      "Error: 403, proxy: 121.236.163.177:888\n",
      "Error: 403, proxy: 58.212.197.222:81\n",
      "Error: 403, proxy: 123.56.216.85:22\n",
      "Error: 403, proxy: 122.70.157.11:808\n",
      "Error: 403, proxy: 47.99.133.26:3128\n",
      "Error: 403, proxy: 221.215.252.88:9999\n",
      "Error: 403, proxy: 112.232.109.1:8118\n",
      "Error: 403, proxy: 58.23.212.10:3129\n",
      "Error: 403, proxy: 139.227.201.203:8118\n",
      "Error: 403, proxy: 222.64.109.23:9000\n",
      "Error: 403, proxy: 111.1.139.96:80\n",
      "Error: 200, proxy: 175.44.46.3:8118\n",
      "Error: 403, proxy: 117.147.111.210:3128\n",
      "Error: 403, proxy: 123.56.21.99:22\n",
      "0 successes in pass, time taken 60.98807954788208, 8885 movies to go\n"
     ]
    }
   ],
   "source": [
    "# using a parallel instead of sequential approach in 00c\n",
    "async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=60)) as session:\n",
    "    while len(celebrities) > 0:\n",
    "        start_time = time.time()\n",
    "        celeb_it = iter(celebrities)\n",
    "        futures = []\n",
    "        for p in proxies:\n",
    "            futures.append(get_celeb(p, next(celeb_it), session))\n",
    "        results = await asyncio.gather(*futures)\n",
    "        print(f\"{sum(results)} successes in pass, time taken {time.time() - start_time}, {len(celebrities)} movies to go\")\n",
    "        time.sleep(60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "async def test_proxy(session, proxy):\n",
    "    try:\n",
    "        async with session.get(\"https://movie.douban.com\", proxy=f\"http://{proxy}\") as resp:\n",
    "            if resp.status == 200:\n",
    "                result = await resp.text()\n",
    "                if not result.startswith(\"<script>\"):\n",
    "                    print(f\"Found working proxy {proxy}\")\n",
    "                    return proxy\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "async def get_working_proxies():\n",
    "    futures = []\n",
    "    session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=60))\n",
    "\n",
    "    # issue requests to all proxies in parallel\n",
    "    for p in proxies:\n",
    "        futures.append(test_proxy(session, p))\n",
    "\n",
    "    is_working = await asyncio.gather(*futures)\n",
    "    working = [i for i in is_working if i is not None]\n",
    "\n",
    "    await session.close()\n",
    "    print(f\"Finished checking working proxies, list size {len(working)}\")\n",
    "    return working"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cf. 00c\n",
    "\n",
    "def get_next_proxy():\n",
    "    try:\n",
    "        return f\"http://{next(get_next_proxy.proxy_it)}\"\n",
    "    except StopIteration:\n",
    "        get_next_proxy.proxy_it = iter(proxies)\n",
    "        return get_next_proxy()\n",
    "\n",
    "get_next_proxy.proxy_it = iter(proxies)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def get_celeb(celeb, session):\n",
    "    while True:\n",
    "        proxy = get_next_proxy()\n",
    "        try:\n",
    "            async with session.get(\"https://movie.douban.com\" + celeb, proxy=f\"http://{proxy}\") as resp:\n",
    "                if resp.status == 200:\n",
    "                    result = await resp.text()\n",
    "                    if not result.startswith(\"<script>\"):\n",
    "                        with open(name, 'w', encoding=\"utf-8\") as f:\n",
    "                            f.write(result)\n",
    "                        break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f\"Scraped {celeb}, proxy {proxy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found working proxy 47.99.133.26:3128\n",
      "Found working proxy 117.10.124.222:8118\n",
      "Found working proxy 222.64.109.23:9000\n",
      "Found working proxy 119.184.185.80:8118\n",
      "Found working proxy 112.232.109.1:8118\n",
      "Found working proxy 113.87.81.187:8118\n",
      "Found working proxy 221.219.103.88:9000\n",
      "Found working proxy 221.215.252.88:9999\n",
      "Found working proxy 39.107.91.243:22\n",
      "Found working proxy 119.8.183.114:3128\n",
      "Found working proxy 116.20.228.108:3128\n",
      "Found working proxy 117.147.111.210:3128\n",
      "Finished checking working proxies, list size 12\n",
      "Exception: Server disconnected\n",
      "Something went wrong, refreshing list of proxies\n",
      "Found working proxy 139.227.201.203:8118\n",
      "Found working proxy 221.215.252.88:9999\n",
      "Found working proxy 47.99.133.26:3128\n",
      "Found working proxy 121.229.132.241:9999\n",
      "Found working proxy 113.87.81.187:8118\n",
      "Found working proxy 222.64.109.23:9000\n",
      "Found working proxy 117.10.124.222:8118\n",
      "Found working proxy 119.8.183.114:3128\n",
      "Found working proxy 116.20.228.108:3128\n",
      "Finished checking working proxies, list size 9\n",
      "Scraped 1313972, proxy 139.227.201.203:8118, 10831 to go\n",
      "Exception: Cannot connect to host 117.10.124.222:8118 ssl:default [Connect call failed ('117.10.124.222', 8118)]\n",
      "Something went wrong, refreshing list of proxies\n",
      "Found working proxy 221.215.252.88:9999\n",
      "Found working proxy 222.64.109.23:9000\n",
      "Found working proxy 117.10.124.222:8118\n",
      "Found working proxy 113.87.81.187:8118\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mCancelledError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mException: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSomething went wrong, refreshing list of proxies\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 30\u001B[0m     working_proxies \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m get_working_proxies()\n\u001B[0;32m     31\u001B[0m     refresh_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScraped \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, proxy \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproxy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(celebrities) \u001B[38;5;241m-\u001B[39m count\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to go\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mget_working_proxies\u001B[1;34m()\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m proxies:\n\u001B[0;32m     19\u001B[0m     futures\u001B[38;5;241m.\u001B[39mappend(test_proxy(session, p))\n\u001B[1;32m---> 21\u001B[0m is_working \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m*\u001B[39mfutures)\n\u001B[0;32m     22\u001B[0m working \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m is_working \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m session\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[1;31mCancelledError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "working_proxies = await get_working_proxies()\n",
    "refresh_time = time.time()\n",
    "count = 0\n",
    "\n",
    "async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=60)) as session:\n",
    "    for c in celebrities:\n",
    "        i = c.split('/')[2]\n",
    "        name = f\"./data/celeb/{i}.html\"\n",
    "        count += 1\n",
    "        if os.path.isfile(name):\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            # select random from working proxies\n",
    "            proxy = working_proxies[random.randrange(0, len(working_proxies))]\n",
    "            try:\n",
    "                async with session.get(\"https://movie.douban.com\" + c, proxy=f\"http://{proxy}\") as resp:\n",
    "                    if resp.status == 200:\n",
    "                        result = await resp.text()\n",
    "                        if not result.startswith(\"<script>\"):\n",
    "                            with open(name, 'w', encoding=\"utf-8\") as f:\n",
    "                                f.write(result)\n",
    "                            break\n",
    "                        print(\"Response starts with script tag\")\n",
    "                    else:\n",
    "                        print(f\"Wrong response code: {resp.status}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e}\")\n",
    "            print(f\"Something went wrong, refreshing list of proxies\")\n",
    "            working_proxies = await get_working_proxies()\n",
    "            refresh_time = time.time()\n",
    "\n",
    "        print(f\"Scraped {i}, proxy {proxy}, {len(celebrities) - count} to go\")\n",
    "\n",
    "        # refresh list of proxies every half an hour\n",
    "        if time.time() - refresh_time > 1800:\n",
    "            print(\"Refreshing list of proxies\")\n",
    "            working_proxies = await get_working_proxies()\n",
    "            refresh_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "1904"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of scraped files is correct\n",
    "_, _, files = next(os.walk(\"./data/celeb\"))\n",
    "len(files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}